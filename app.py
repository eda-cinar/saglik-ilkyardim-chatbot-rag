import streamlit as st
import os
import google.generativeai as genai
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# API AnahtarÄ± AyarÄ± (Sadece gÃ¼venilir bir Streamlit Secrets kullanÄ±mÄ± iÃ§in)
# genai.configure(api_key=os.getenv("GOOGLE_API_KEY")) # ArtÄ±k LLM/Embeddings sÄ±nÄ±fÄ±na direkt verilecek

st.set_page_config(page_title="Ä°lk YardÄ±m Chatbotu", page_icon="ğŸ’¬")
st.title("ğŸ¥ SaÄŸlÄ±k ve Ä°lk YardÄ±m Chatbotu")
st.write("RAG mimarili yapay zeka destekli saÄŸlÄ±k asistanÄ±na hoÅŸ geldiniz!")

# --- FAISS VE EMBEDDINGS OLUÅTURMA (SADECE BÄ°R KEZ Ã‡ALIÅIR) ---
@st.cache_resource
def setup_rag_environment():
    # 1. Veri YÃ¼kleme ve ParÃ§alama
    data_files = ["data/ilk_yardim_bilgileri.txt", "data/saglik_onerileri.txt", "data/acil_durumlar.txt"]
    raw_text = ""
    for file in data_files:
        try:
            with open(file, "r", encoding="utf-8") as f:
                raw_text += f.read() + "\n"
        except FileNotFoundError:
            # Gerekirse bu uyarÄ±yÄ± kullanÄ±cÄ±ya gÃ¶sterebiliriz
            pass

    text_splitter = CharacterTextSplitter(
        separator="\n", chunk_size=500, chunk_overlap=100, length_function=len
    )
    texts = text_splitter.split_text(raw_text)

    # 2. Embedding Modeli ve VektÃ¶r VeritabanÄ± OluÅŸturma
    # API key, os.getenv'den otomatik olarak okunur.
    embeddings = GoogleGenerativeAIEmbeddings(model="embedding-001") 
    vectorstore = FAISS.from_texts(texts, embeddings)
    
    return vectorstore

# VektÃ¶r veritabanÄ±nÄ± oluÅŸtur veya cache'den yÃ¼kle
try:
    vectorstore = setup_rag_environment()
except Exception as e:
    st.error("RAG ortamÄ± baÅŸlatÄ±lamadÄ±. API AnahtarÄ±nÄ±zÄ± ve Billing ayarlarÄ±nÄ±zÄ± kontrol edin.")
    st.exception(e)
    st.stop()


# --- CHATBOT MANTIÄI (rag_answer fonksiyonu app.py'ye taÅŸÄ±ndÄ±) ---
def rag_answer(query, vectorstore):
    # LLM Modelini TanÄ±mlama
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0.3
        # API key ortam deÄŸiÅŸkeninden alÄ±nacak
    )
    
    # 1. Retrieval (Arama) BileÅŸenini TanÄ±mlama
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
    
    # 2. Prompt Template (Ä°stemi) OluÅŸturma
    prompt_template = """AÅŸaÄŸÄ±daki baÄŸlam bilgileri, ilk yardÄ±m ve saÄŸlÄ±k konularÄ±nda hazÄ±rlanmÄ±ÅŸtÄ±r. 
    Verilen baÄŸlamÄ± kullanarak, kullanÄ±cÄ± sorusuna net ve gÃ¼venilir bir TÃ¼rkÃ§e yanÄ±t ver. 
    BaÄŸlamda bulunmayan bir bilgi sorulursa, "Verilen bilgilerde bu konu hakkÄ±nda bilgi bulunmamaktadÄ±r." diye yanÄ±tla.

    BAÄLAM:
    {context}

    SORU: {question}
    YANIT:"""

    PROMPT = PromptTemplate(
        template=prompt_template, input_variables=["context", "question"]
    )
    
    # 3. RAG Zincirini OluÅŸturma (Retrieval + Generation)
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=False,
        chain_type_kwargs={"prompt": PROMPT}
    )
    
    response = qa_chain.run(query)
    
    return response


# --- KULLANICI ARAYÃœZÃœ ---
user_input = st.text_input("Sorunuzu yazÄ±n (Ã¶rn: Elimi kestim, ne yapmalÄ±yÄ±m?):")

if st.button("GÃ¶nder"):
    if user_input.strip():
        # rag_answer ÅŸimdi vectorstore'u alÄ±yor
        with st.spinner('YanÄ±t oluÅŸturuluyor...'):
            response = rag_answer(user_input, vectorstore)
        st.success(response)
    else:
        st.warning("LÃ¼tfen bir soru girin.")
